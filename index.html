<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Whisper Transcriber & Translator</title>
  <!-- WaveSurfer.js for audio visualization and editing - using v5 for better compatibility -->
  <script src="https://unpkg.com/wavesurfer.js@5.2.0/dist/wavesurfer.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@5.2.0/dist/plugin/wavesurfer.regions.min.js"></script>
  <style>
    body { font-family: Arial, sans-serif; max-width: 900px; margin: 2rem auto; padding: 1rem; }
    label { display: block; margin: 1rem 0 0.5rem; }
    button { margin: 0.5rem 0.5rem 0.5rem 0; padding: 0.5rem 1rem; cursor: pointer; }
    .button-primary { background-color: #4CAF50; color: white; border: none; border-radius: 4px; }
    .button-secondary { background-color: #3498db; color: white; border: none; border-radius: 4px; }
    .button-disabled { background-color: #cccccc; color: #666666; border: none; border-radius: 4px; cursor: not-allowed; }
    #log { white-space: pre-wrap; background: #f4f4f4; padding: 1rem; border-radius: 4px; max-height: 200px; overflow-y: scroll; }
    .progress-container { margin-top: 1rem; display: none; }
    progress { width: 100%; }
    #waveform { width: 100%; height: 128px; margin: 1rem 0; }
    #audio-controls { margin-bottom: 1rem; }
    .download-section { margin: 1rem 0; }
    .editor-section { margin: 1rem 0; padding: 1rem; border: 1px solid #ddd; border-radius: 4px; }
    .time-display { font-family: monospace; margin: 0.5rem 0; }
    .chunk-marker { position: absolute; width: 2px; background-color: red; height: 100%; z-index: 10; }
    #chunks-container { margin-top: 1rem; display: none; }
    #chunks-info { font-family: monospace; }
    .option-row { margin: 1rem 0; }
    .settings-section { margin: 1rem 0; padding: 1rem; border: 1px solid #ddd; border-radius: 4px; }
  </style>
</head>
<body>
  <h1>Whisper Transcriber & Translator</h1>
  <label for="apiKey">OpenAI API Key:</label>
  <input type="password" id="apiKey" placeholder="sk-..." style="width: 100%;" />

  <div class="editor-section">
    <h2>Audio Editor</h2>
    <label for="mediaFile">Audio/Video File:</label>
    <input type="file" id="mediaFile" accept="audio/*,video/*" />
    
    <div id="waveform"></div>
    
    <div class="time-display">
      Selection: <span id="selection-start">0:00</span> - <span id="selection-end">0:00</span> 
      (Duration: <span id="selection-duration">0:00</span>)
    </div>
    
    <div id="audio-controls">
      <button id="playBtn" class="button-secondary" disabled>Play</button>
      <button id="pauseBtn" class="button-secondary" disabled>Pause</button>
      <button id="zoomInBtn" class="button-secondary" disabled>Zoom In</button>
      <button id="zoomOutBtn" class="button-secondary" disabled>Zoom Out</button>
      <button id="createRegionBtn" class="button-secondary" disabled>Create Selection</button>
      <button id="clearRegionsBtn" class="button-secondary" disabled>Clear Selection</button>
    </div>
  </div>

  <div class="settings-section">
    <h2>Auto-Chunking Settings</h2>
    <div class="option-row">
      <label for="silenceThreshold">Silence Threshold (-100 to 0 dB):</label>
      <input type="range" id="silenceThreshold" min="-100" max="0" value="-40" step="1">
      <span id="silenceThresholdValue">-40 dB</span>
    </div>
    
    <div class="option-row">
      <label for="minSilenceDuration">Minimum Silence Duration (milliseconds):</label>
      <input type="range" id="minSilenceDuration" min="100" max="2000" value="700" step="100">
      <span id="minSilenceDurationValue">700 ms</span>
    </div>
    
    <div class="option-row">
      <button id="detectChunksBtn" class="button-secondary" disabled>Detect Chunks</button>
      <span id="chunksStatus"></span>
    </div>
  </div>
  
  <div id="chunks-container">
    <h3>Detected Audio Chunks</h3>
    <div id="chunks-info"></div>
  </div>

  <div class="progress-container" id="progressContainer">
    <p>Uploading: <span id="progressPercent">0%</span></p>
    <progress id="progressBar" value="0" max="100"></progress>
  </div>

  <button id="transcribeBtn" class="button-primary">Transcribe Selected Region</button>
  <button id="transcribeAllChunksBtn" class="button-primary" disabled>Transcribe All Chunks</button>
  <button id="translateBtn" class="button-primary" disabled>Translate</button>

  <div class="download-section">
    <h2>Downloads</h2>
    <button id="downloadSrtBtn" class="button-disabled" disabled>Download Transcript (.srt)</button>
    <button id="downloadTranslatedSrtBtn" class="button-disabled" disabled>Download Translation (.srt)</button>
  </div>

  <h2>Log</h2>
  <div id="log"></div>

  <script>
    let originalSrt = '';
    let translatedSrt = '';
    let wavesurfer = null;
    let selectedRegion = null;
    let audioBlob = null;
    let detectedChunks = [];
    let audioContext = null;
    let audioBuffer = null;
    let isProcessingChunks = false;
    
    const logDiv = document.getElementById('log');
    const progressContainer = document.getElementById('progressContainer');
    const progressBar = document.getElementById('progressBar');
    const progressPercent = document.getElementById('progressPercent');
    const downloadSrtBtn = document.getElementById('downloadSrtBtn');
    const downloadTranslatedSrtBtn = document.getElementById('downloadTranslatedSrtBtn');
    const selectionStart = document.getElementById('selection-start');
    const selectionEnd = document.getElementById('selection-end');
    const selectionDuration = document.getElementById('selection-duration');
    const chunksContainer = document.getElementById('chunks-container');
    const chunksInfo = document.getElementById('chunks-info');
    const detectChunksBtn = document.getElementById('detectChunksBtn');
    const transcribeAllChunksBtn = document.getElementById('transcribeAllChunksBtn');
    const silenceThreshold = document.getElementById('silenceThreshold');
    const silenceThresholdValue = document.getElementById('silenceThresholdValue');
    const minSilenceDuration = document.getElementById('minSilenceDuration');
    const minSilenceDurationValue = document.getElementById('minSilenceDurationValue');
    const chunksStatus = document.getElementById('chunksStatus');

    // Update slider values display
    silenceThreshold.addEventListener('input', () => {
      silenceThresholdValue.textContent = `${silenceThreshold.value} dB`;
    });
    
    minSilenceDuration.addEventListener('input', () => {
      minSilenceDurationValue.textContent = `${minSilenceDuration.value} ms`;
    });

    function log(msg) {
      logDiv.textContent += msg + '\n';
      logDiv.scrollTop = logDiv.scrollHeight;
    }
    
    function formatTime(seconds) {
      const minutes = Math.floor(seconds / 60);
      const secs = Math.floor(seconds % 60);
      return `${minutes}:${secs.toString().padStart(2, '0')}`;
    }
    
    function formatTimeExact(seconds) {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const ms = Math.floor((seconds % 1) * 1000);
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
    }
    
    // Initialize WaveSurfer with v5 API
    function initWaveSurfer() {
      // Create wavesurfer with regions plugin
      wavesurfer = WaveSurfer.create({
        container: '#waveform',
        waveColor: '#3498db',
        progressColor: '#2980b9',
        cursorColor: '#333',
        barWidth: 2,
        barRadius: 3,
        cursorWidth: 1,
        height: 100,
        barGap: 2,
        responsive: true,
        plugins: [
          WaveSurfer.regions.create({
            regions: [],
            dragSelection: true,
            color: 'rgba(76, 175, 80, 0.3)'
          })
        ]
      });
      
      // Enable controls once audio is loaded
      wavesurfer.on('ready', function() {
        document.getElementById('playBtn').disabled = false;
        document.getElementById('pauseBtn').disabled = false;
        document.getElementById('zoomInBtn').disabled = false;
        document.getElementById('zoomOutBtn').disabled = false;
        document.getElementById('createRegionBtn').disabled = false;
        document.getElementById('clearRegionsBtn').disabled = false;
        detectChunksBtn.disabled = false;
        
        // Initially create a region for the entire audio
        const duration = wavesurfer.getDuration();
        selectedRegion = wavesurfer.addRegion({
          start: 0,
          end: duration,
          color: 'rgba(76, 175, 80, 0.3)'
        });
        
        updateSelectionDisplay(0, duration);
      });
      
      // Update selection time when region is updated
      wavesurfer.on('region-update-end', function(region) {
        selectedRegion = region;
        updateSelectionDisplay(region.start, region.end);
      });
      
      // Save created regions
      wavesurfer.on('region-created', function(region) {
        // Remove previous regions
        const regions = wavesurfer.regions.list;
        Object.keys(regions).forEach(id => {
          if (id !== region.id) {
            regions[id].remove();
          }
        });
        
        selectedRegion = region;
        updateSelectionDisplay(region.start, region.end);
      });
    }
    
    function updateSelectionDisplay(start, end) {
      selectionStart.textContent = formatTime(start);
      selectionEnd.textContent = formatTime(end);
      selectionDuration.textContent = formatTime(end - start);
    }
    
    // Handle file selection
    document.getElementById('mediaFile').addEventListener('change', async function(e) {
      const file = e.target.files[0];
      if (!file) return;
      
      // Reset chunks
      detectedChunks = [];
      chunksContainer.style.display = 'none';
      transcribeAllChunksBtn.disabled = true;
      
      // If wavesurfer hasn't been initialized yet, initialize it
      if (!wavesurfer) {
        initWaveSurfer();
      } else {
        // Clear existing regions
        wavesurfer.clearRegions();
      }
      
      // Load the file into wavesurfer
      const fileURL = URL.createObjectURL(file);
      wavesurfer.load(fileURL);
      log(`Loaded file: ${file.name}`);
      
      // Also load the file into an AudioBuffer for analysis
      try {
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        
        const arrayBuffer = await file.arrayBuffer();
        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        log('Audio data loaded for analysis');
      } catch (error) {
        log(`Error loading audio for analysis: ${error.message}`);
      }
    });
    
    // Audio control buttons
    document.getElementById('playBtn').addEventListener('click', () => {
      if (selectedRegion) {
        selectedRegion.play();
      } else {
        wavesurfer.play();
      }
    });
    
    document.getElementById('pauseBtn').addEventListener('click', () => {
      wavesurfer.pause();
    });
    
    document.getElementById('zoomInBtn').addEventListener('click', () => {
      const currentZoom = wavesurfer.params.minPxPerSec || 1;
      wavesurfer.zoom(currentZoom * 1.5);
    });
    
    document.getElementById('zoomOutBtn').addEventListener('click', () => {
      const currentZoom = wavesurfer.params.minPxPerSec || 1;
      wavesurfer.zoom(Math.max(1, currentZoom / 1.5));
    });
    
    document.getElementById('createRegionBtn').addEventListener('click', () => {
      wavesurfer.clearRegions();
      selectedRegion = wavesurfer.addRegion({
        start: wavesurfer.getCurrentTime(),
        end: Math.min(wavesurfer.getCurrentTime() + 30, wavesurfer.getDuration()),
        color: 'rgba(76, 175, 80, 0.3)'
      });
      
      updateSelectionDisplay(selectedRegion.start, selectedRegion.end);
    });
    
    document.getElementById('clearRegionsBtn').addEventListener('click', () => {
      wavesurfer.clearRegions();
      selectedRegion = wavesurfer.addRegion({
        start: 0,
        end: wavesurfer.getDuration(),
        color: 'rgba(76, 175, 80, 0.3)'
      });
      
      updateSelectionDisplay(0, wavesurfer.getDuration());
    });
    
    // Function to detect silence for intelligent chunking
    async function detectSilence() {
      if (!audioBuffer) {
        log('No audio loaded for analysis');
        return [];
      }
      
      chunksStatus.textContent = 'Analyzing...';
      
      // Get raw audio data (we'll use the first channel if stereo)
      const audioData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      
      // Get threshold values from UI sliders
      const thresholdDb = parseInt(silenceThreshold.value);
      const minSilenceMs = parseInt(minSilenceDuration.value);
      
      // Convert from dB to amplitude ratio (0 to 1)
      const thresholdAmplitude = Math.pow(10, thresholdDb / 20);
      
      // Convert from ms to samples
      const minSilenceSamples = Math.floor(minSilenceMs * sampleRate / 1000);
      
      const silencePoints = [];
      let inSilence = false;
      let silenceStart = 0;
      let consecutiveSilenceSamples = 0;
      
      // Process in chunks to not freeze the UI
      const CHUNK_SIZE = 50000;
      let processedSamples = 0;
      
      while (processedSamples < audioData.length) {
        const endIndex = Math.min(processedSamples + CHUNK_SIZE, audioData.length);
        
        for (let i = processedSamples; i < endIndex; i++) {
          const amplitude = Math.abs(audioData[i]);
          
          if (amplitude < thresholdAmplitude) {
            // We're in a silent part
            if (!inSilence) {
              // Just entered silence
              silenceStart = i;
              inSilence = true;
            }
            consecutiveSilenceSamples++;
          } else {
            // We're in a non-silent part
            if (inSilence) {
              // Just exited silence, check if it was long enough
              if (consecutiveSilenceSamples >= minSilenceSamples) {
                // This silence was long enough, record its position
                const silenceMiddle = silenceStart + (consecutiveSilenceSamples / 2);
                silencePoints.push(silenceMiddle / sampleRate);
              }
              inSilence = false;
            }
            consecutiveSilenceSamples = 0;
          }
        }
        
        processedSamples = endIndex;
        
        // Let the UI update
        await new Promise(resolve => setTimeout(resolve, 0));
        chunksStatus.textContent = `Analyzing... ${Math.round(processedSamples / audioData.length * 100)}%`;
      }
      
      // In case we end in silence
      if (inSilence && consecutiveSilenceSamples >= minSilenceSamples) {
        const silenceMiddle = silenceStart + (consecutiveSilenceSamples / 2);
        silencePoints.push(silenceMiddle / sampleRate);
      }
      
      return silencePoints;
    }
    
    // Function to create optimized chunks that stay under 25MB
    async function createOptimizedChunks() {
      if (!audioBuffer || !wavesurfer) {
        return [];
      }
      
      // Find all silence points
      const silencePoints = await detectSilence();
      
      if (silencePoints.length === 0) {
        log('No significant pauses detected. Try adjusting the silence threshold or minimum duration.');
        return [];
      }
      
      log(`Detected ${silencePoints.length} potential break points`);
      
      // Get the audio duration
      const totalDuration = audioBuffer.duration;
      
      // Estimate the bytes per second rate (rough approximation)
      // Assuming 16bit stereo at 44.1kHz for WAV
      const bytesPerSecond = 44100 * 4; // 44.1kHz * (2 channels * 2 bytes per sample)
      
      // Target size in bytes (slightly under 25MB to be safe)
      const targetChunkSizeBytes = 23 * 1024 * 1024;
      
      // Calculate target duration per chunk
      const targetChunkDuration = targetChunkSizeBytes / bytesPerSecond;
      
      // Create chunks
      const chunks = [];
      let chunkStart = 0;
      
      while (chunkStart < totalDuration) {
        // Target end time for this chunk (min of target duration or end)
        const idealEnd = Math.min(chunkStart + targetChunkDuration, totalDuration);
        
        // Find the closest silence point to the ideal end
        let bestBreakPoint = idealEnd;
        let minDistance = Number.MAX_VALUE;
        
        for (const silencePoint of silencePoints) {
          // Only consider silence points past our starting point and not too far past ideal end
          if (silencePoint > chunkStart && silencePoint < idealEnd + 30) {
            const distance = Math.abs(silencePoint - idealEnd);
            if (distance < minDistance) {
              minDistance = distance;
              bestBreakPoint = silencePoint;
            }
          }
        }
        
        // If we couldn't find a good break point, just use the ideal end
        if (bestBreakPoint === idealEnd && idealEnd < totalDuration) {
          log(`Warning: No natural pause found near the ${formatTime(idealEnd)} mark`);
        }
        
        // Add the chunk
        chunks.push({
          start: chunkStart,
          end: bestBreakPoint,
          duration: bestBreakPoint - chunkStart
        });
        
        // Move to next chunk
        chunkStart = bestBreakPoint;
      }
      
      return chunks;
    }
    
    // Show the detected chunks and visualize them
    function displayChunks(chunks) {
      if (!chunks || chunks.length === 0) {
        chunksContainer.style.display = 'none';
        return;
      }
      
      chunksContainer.style.display = 'block';
      chunksInfo.innerHTML = '';
      
      let totalHTML = '<table style="width:100%; border-collapse: collapse;">';
      totalHTML += '<tr><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Chunk</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Start</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">End</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Duration</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Est. Size</th></tr>';
      
      // Add markers to the waveform
      wavesurfer.clearRegions();
      
      chunks.forEach((chunk, index) => {
        // Add region for this chunk
        wavesurfer.addRegion({
          start: chunk.start,
          end: chunk.end,
          color: 'rgba(76, 175, 80, 0.2)',
          id: `chunk_${index}`
        });
        
        // Add a vertical marker for chunk boundaries (except start)
        if (index > 0) {
          const marker = document.createElement('div');
          marker.className = 'chunk-marker';
          marker.style.left = (chunk.start / wavesurfer.getDuration() * 100) + '%';
          document.querySelector('#waveform').appendChild(marker);
        }
        
        // Calculate estimated size
        const estSizeBytes = chunk.duration * 44100 * 4; // 44.1kHz * (2 channels * 2 bytes per sample)
        const estSizeMB = estSizeBytes / (1024 * 1024);
        
        // Add row to table
        totalHTML += `<tr>
          <td style="border-bottom:1px solid #ddd; padding:5px;">Chunk ${index + 1}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.start)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.end)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.duration)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${estSizeMB.toFixed(2)} MB</td>
        </tr>`;
      });
      
      totalHTML += '</table>';
      chunksInfo.innerHTML = totalHTML;
      
      // Enable the "Transcribe All Chunks" button
      transcribeAllChunksBtn.disabled = false;
    }
    
    detectChunksBtn.addEventListener('click', async () => {
      if (!audioBuffer || !wavesurfer) {
        log('Please load an audio file first');
        return;
      }
      
      // Remove existing chunk markers
      document.querySelectorAll('.chunk-marker').forEach(marker => marker.remove());
      
      log('Detecting optimal chunk boundaries...');
      const chunks = await createOptimizedChunks();
      
      if (chunks.length > 0) {
        log(`Created ${chunks.length} optimized chunks based on silence detection`);
        detectedChunks = chunks;
        displayChunks(chunks);
        chunksStatus.textContent = `${chunks.length} chunks detected`;
      } else {
        chunksStatus.textContent = 'No chunks detected';
      }
    });
    
    // Extract the selected audio region
    async function extractSelectedAudio() {
      if (!wavesurfer || !selectedRegion) {
        alert('Please load an audio file and select a region first.');
        return null;
      }
      
      return new Promise((resolve) => {
        // Get audio element from wavesurfer
        const originalAudio = wavesurfer.media;
        
        // Set up an audio context
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaElementSource(originalAudio.cloneNode());
        
        // Create a destination to save the output
        const destination = audioContext.createMediaStreamDestination();
        source.connect(destination);
        
        // Create a MediaRecorder to capture the output
        const mediaRecorder = new MediaRecorder(destination.stream);
        const chunks = [];
        
        mediaRecorder.ondataavailable = (e) => {
          chunks.push(e.data);
        };
        
        mediaRecorder.onstop = () => {
          const blob = new Blob(chunks, { type: 'audio/wav' });
          audioBlob = blob;
          resolve(blob);
        };
        
        // Start recording and playback from the selected region
        mediaRecorder.start();
        
        // Set the current time to the start of the selected region
        originalAudio.currentTime = selectedRegion.start;
        
        // Play the audio
        originalAudio.play();
        
        // Stop recording when reaching the end of the region
        setTimeout(() => {
          originalAudio.pause();
          mediaRecorder.stop();
        }, (selectedRegion.end - selectedRegion.start) * 1000);
      });
    }
    
    // Extract audio for a specific chunk
    async function extractChunkAudio(chunk) {
      if (!wavesurfer) {
        return null;
      }
      
      // Temporarily create a region for this chunk
      const tempRegion = {
        start: chunk.start,
        end: chunk.end
      };
      
      return new Promise((resolve) => {
        // Get audio element from wavesurfer
        const originalAudio = wavesurfer.media;
        
        // Set up an audio context
        const audioContext = new (window.AudioContext || window.webkitAudioContext)();
        const source = audioContext.createMediaElementSource(originalAudio.cloneNode());
        
        // Create a destination to save the output
        const destination = audioContext.createMediaStreamDestination();
        source.connect(destination);
        
        // Create a MediaRecorder to capture the output
        const mediaRecorder = new MediaRecorder(destination.stream);
        const chunks = [];
        
        mediaRecorder.ondataavailable = (e) => {
          chunks.push(e.data);
        };
        
        mediaRecorder.onstop = () => {
          const blob = new Blob(chunks, { type: 'audio/wav' });
          resolve(blob);
        };
        
        // Start recording and playback from the selected region
        mediaRecorder.start();
        
        // Set the current time to the start of the chunk
        originalAudio.currentTime = tempRegion.start;
        
        // Play the audio
        originalAudio.play();
        
        // Stop recording when reaching the end of the region
        setTimeout(() => {
          originalAudio.pause();
          mediaRecorder.stop();
        }, (tempRegion.end - tempRegion.start) * 1000);
      });
    }

    async function transcribe() {
      const apiKey = document.getElementById('apiKey').value.trim();
      
      if (!apiKey) {
        alert('Please provide an OpenAI API key.');
        return;
      }
      
      if (!wavesurfer || !selectedRegion) {
        alert('Please load an audio file and select a region first.');
        return;
      }
      
      try {
        log('Extracting selected region...');
        const extractedAudio = await extractSelectedAudio();
        
        if (!extractedAudio) {
          log('Failed to extract audio region.');
          return;
        }
        
        log(`Extracted audio size: ${Math.round(extractedAudio.size / 1024 / 1024 * 100) / 100} MB`);
        
        // Check file size - OpenAI has a 25MB limit
        if (extractedAudio.size > 25 * 1024 * 1024) {
          alert('Selected region still exceeds 25MB limit. Please select a smaller region or use auto-chunking.');
          return;
        }

        const form = new FormData();
        form.append('file', extractedAudio, 'audio.wav');
        form.append('model', 'whisper-1');
        form.append('response_format', 'srt');

        log('Sending to Whisper API...');
        progressContainer.style.display = 'block';
        
        // Create a custom fetch with upload progress
        const xhr = new XMLHttpRequest();
        xhr.open('POST', 'https://api.openai.com/v1/audio/transcriptions');
        xhr.setRequestHeader('Authorization', `Bearer ${apiKey}`);
        
        xhr.upload.onprogress = (event) => {
          if (event.lengthComputable) {
            const percentComplete = Math.round((event.loaded / event.total) * 100);
            progressBar.value = percentComplete;
            progressPercent.textContent = percentComplete + '%';
          }
        };
        
        xhr.onload = function() {
          progressContainer.style.display = 'none';
          
          if (xhr.status === 200) {
            originalSrt = xhr.responseText;
            log('Received SRT.');
            
            // Enable the download button and translate button
            downloadSrtBtn.disabled = false;
            downloadSrtBtn.classList.remove('button-disabled');
            downloadSrtBtn.classList.add('button-secondary');
            
            document.getElementById('translateBtn').disabled = false;
          } else {
            log('Error: ' + xhr.responseText);
          }
        };
        
        xhr.onerror = function() {
          progressContainer.style.display = 'none';
          log('Network error occurred');
        };
        
        xhr.send(form);
      } catch (error) {
        log('Error: ' + error.message);
      }
    }
    
    // Transcribe all chunks and merge them
    async function transcribeAllChunks() {
      if (isProcessingChunks) {
        alert('Already processing chunks. Please wait.');
        return;
      }
      
      if (!detectedChunks || detectedChunks.length === 0) {
        alert('No chunks detected. Please run the chunk detection first.');
        return;
      }
      
      const apiKey = document.getElementById('apiKey').value.trim();
      if (!apiKey) {
        alert('Please provide an OpenAI API key.');
        return;
      }
      
      isProcessingChunks = true;
      
      try {
        const allChunksSrt = [];
        progressContainer.style.display = 'block';
        
        for (let i = 0; i < detectedChunks.length; i++) {
          const chunk = detectedChunks[i];
          log(`Processing chunk ${i+1}/${detectedChunks.length} (${formatTime(chunk.start)} - ${formatTime(chunk.end)})...`);
          
          // Extract the audio for this chunk
          const chunkAudio = await extractChunkAudio(chunk);
          
          if (!chunkAudio) {
            log(`Failed to extract audio for chunk ${i+1}`);
            continue;
          }
          
          log(`Chunk ${i+1} size: ${Math.round(chunkAudio.size / 1024 / 1024 * 100) / 100} MB`);
          
          // Create form data for this chunk
          const form = new FormData();
          form.append('file', chunkAudio, `chunk${i+1}.wav`);
          form.append('model', 'whisper-1');
          form.append('response_format', 'srt');
          
          // Send to Whisper API
          log(`Transcribing chunk ${i+1}...`);
          progressPercent.textContent = `Chunk ${i+1}/${detectedChunks.length}`;
          progressBar.value = (i / detectedChunks.length) * 100;
          
          // Use fetch for simplicity
          const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${apiKey}`
            },
            body: form
          });
          
          if (!response.ok) {
            const errorText = await response.text();
            log(`Error transcribing chunk ${i+1}: ${errorText}`);
            continue;
          }
          
          const chunkSrt = await response.text();
          
          // Store the SRT with timing information
          allChunksSrt.push({
            srt: chunkSrt,
            offset: chunk.start
          });
          
          log(`Chunk ${i+1} transcribed successfully`);
          
          // Wait a moment between chunks to avoid rate limiting
          await new Promise(resolve => setTimeout(resolve, 500));
        }
        
        // Merge all SRTs with correct timings
        log('Merging all transcriptions...');
        originalSrt = mergeSubtitles(allChunksSrt);
        
        log('All chunks processed and merged successfully');
        
        // Enable buttons
        downloadSrtBtn.disabled = false;
        downloadSrtBtn.classList.remove('button-disabled');
        downloadSrtBtn.classList.add('button-secondary');
        document.getElementById('translateBtn').disabled = false;
        
      } catch (error) {
        log(`Error processing chunks: ${error.message}`);
      } finally {
        isProcessingChunks = false;
        progressContainer.style.display = 'none';
      }
    }
    
    // Merge multiple SRT files with proper timing offsets
    function mergeSubtitles(srtChunks) {
      if (!srtChunks || srtChunks.length === 0) return '';
      
      let allSubs = [];
      let subCounter = 1;
      
      // Process each chunk
      srtChunks.forEach(chunk => {
        const { srt, offset } = chunk;
        
        // Parse the SRT content
        const subLines = srt.trim().split('\n\n');
        
        // Process each subtitle in this chunk
        for (const sub of subLines) {
          const lines = sub.trim().split('\n');
          
          // Skip empty subtitles
          if (lines.length < 3) continue;
          
          // Skip subtitle number (lines[0])
          const timeStr = lines[1];
          
          // Parse start and end times
          const times = timeStr.split(' --> ');
          if (times.length !== 2) continue;
          
          const startTime = parseSrtTime(times[0]) + offset;
          const endTime = parseSrtTime(times[1]) + offset;
          
          // Create adjusted subtitle
          const text = lines.slice(2).join('\n');
          
          allSubs.push({
            number: subCounter++,
            start: startTime,
            end: endTime,
            text: text
          });
        }
      });
      
      // Sort subtitles by start time
      allSubs.sort((a, b) => a.start - b.start);
      
      // Format as SRT
      let result = '';
      for (const sub of allSubs) {
        result += `${sub.number}\n`;
        result += `${formatSrtTime(sub.start)} --> ${formatSrtTime(sub.end)}\n`;
        result += `${sub.text}\n\n`;
      }
      
      return result;
    }
    
    // Parse SRT time format to seconds
    function parseSrtTime(timeStr) {
      const [time, ms] = timeStr.split(',');
      const [hours, minutes, seconds] = time.split(':').map(Number);
      return hours * 3600 + minutes * 60 + seconds + parseInt(ms) / 1000;
    }
    
    // Format seconds to SRT time format
    function formatSrtTime(seconds) {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const ms = Math.floor((seconds % 1) * 1000);
      
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
    }

    async function translateSrt() {
      const apiKey = document.getElementById('apiKey').value.trim();
      if (!originalSrt) return;
      
      log('Translating SRT with GPT-4o...');
      const systemMsg = 'You are a helpful assistant that translates subtitles while preserving the timing and SRT structure exactly.';
      const userMsg = `Translate the following SRT to the target language, preserving numbering and timestamps exactly:\n\n${originalSrt}`;
      const payload = {
        model: 'gpt-4o', 
        messages: [
          { role: 'system', content: systemMsg },
          { role: 'user', content: userMsg }
        ], 
        max_tokens: 20000
      };
      const resp = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST', 
        headers: {
          'Content-Type': 'application/json', 
          'Authorization': `Bearer ${apiKey}`
        }, 
        body: JSON.stringify(payload)
      });
      if (!resp.ok) {
        const err = await resp.text(); 
        log('Error: ' + err); 
        return;
      }
      const data = await resp.json();
      translatedSrt = data.choices[0].message.content;
      log('Translation done.');
      
      // Enable the download button
      downloadTranslatedSrtBtn.disabled = false;
      downloadTranslatedSrtBtn.classList.remove('button-disabled');
      downloadTranslatedSrtBtn.classList.add('button-secondary');
    }
    
    // Download buttons event listeners
    downloadSrtBtn.addEventListener('click', () => {
      if (!originalSrt) return;
      
      const url = URL.createObjectURL(new Blob([originalSrt], { type: 'text/plain' }));
      const a = document.createElement('a');
      a.href = url;
      a.download = 'transcript.srt';
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
    });
    
    downloadTranslatedSrtBtn.addEventListener('click', () => {
      if (!translatedSrt) return;
      
      const url = URL.createObjectURL(new Blob([translatedSrt], { type: 'text/plain' }));
      const a = document.createElement('a');
      a.href = url;
      a.download = 'translated.srt';
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
    });

    document.getElementById('transcribeBtn').addEventListener('click', transcribe);
    document.getElementById('transcribeAllChunksBtn').addEventListener('click', transcribeAllChunks);
    document.getElementById('translateBtn').addEventListener('click', translateSrt);
  </script>
</body>
</html>
