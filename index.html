<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Whisper Transcriber & Translator</title>
  <!-- WaveSurfer.js v6 which uses AudioWorkletNode instead of ScriptProcessorNode -->
  <script src="https://unpkg.com/wavesurfer.js@6.6.3/dist/wavesurfer.min.js"></script>
  <script src="https://unpkg.com/wavesurfer.js@6.6.3/dist/plugin/wavesurfer.regions.min.js"></script>
  <style>
    body { font-family: Arial, sans-serif; max-width: 900px; margin: 2rem auto; padding: 1rem; }
    label { display: block; margin: 1rem 0 0.5rem; }
    button { margin: 0.5rem 0.5rem 0.5rem 0; padding: 0.5rem 1rem; cursor: pointer; }
    .button-primary { background-color: #4CAF50; color: white; border: none; border-radius: 4px; }
    .button-secondary { background-color: #3498db; color: white; border: none; border-radius: 4px; }
    .button-disabled { background-color: #cccccc; color: #666666; border: none; border-radius: 4px; cursor: not-allowed; }
    #log { white-space: pre-wrap; background: #f4f4f4; padding: 1rem; border-radius: 4px; max-height: 200px; overflow-y: scroll; }
    .progress-container { margin-top: 1rem; display: none; }
    progress { width: 100%; }
    #waveform { width: 100%; height: 128px; margin: 1rem 0; }
    .download-section { margin: 1rem 0; }
    .editor-section { margin: 1rem 0; padding: 1rem; border: 1px solid #ddd; border-radius: 4px; }
    .chunk-marker { position: absolute; width: 2px; background-color: red; height: 100%; z-index: 10; }
    #chunks-container { margin-top: 1rem; display: none; }
    #chunks-info { font-family: monospace; }
    .option-row { margin: 1rem 0; }
    .settings-section { margin: 1rem 0; padding: 1rem; border: 1px solid #ddd; border-radius: 4px; }
  </style>
</head>
<body>
  <h1>Whisper Transcriber & Translator</h1>
  <label for="apiKey">OpenAI API Key:</label>
  <input type="password" id="apiKey" placeholder="sk-..." style="width: 100%;" />

  <div class="editor-section">
    <h2>Audio Visualization</h2>
    <label for="mediaFile">Audio/Video File:</label>
    <input type="file" id="mediaFile" accept="audio/*,video/*" />
    
    <div id="waveform"></div>
    
    <div id="audio-controls">
      <button id="playBtn" class="button-secondary" disabled>Play</button>
      <button id="pauseBtn" class="button-secondary" disabled>Pause</button>
      <button id="zoomInBtn" class="button-secondary" disabled>Zoom In</button>
      <button id="zoomOutBtn" class="button-secondary" disabled>Zoom Out</button>
    </div>
  </div>

  <div class="settings-section">
    <h2>Auto-Chunking Settings</h2>
    <div class="option-row">
      <label for="silenceThreshold">Silence Threshold (-100 to 0 dB):</label>
      <input type="range" id="silenceThreshold" min="-100" max="0" value="-40" step="1">
      <span id="silenceThresholdValue">-40 dB</span>
    </div>
    
    <div class="option-row">
      <label for="minSilenceDuration">Minimum Silence Duration (milliseconds):</label>
      <input type="range" id="minSilenceDuration" min="100" max="2000" value="700" step="100">
      <span id="minSilenceDurationValue">700 ms</span>
    </div>
    
    <div class="option-row">
      <button id="detectChunksBtn" class="button-secondary" disabled>Detect Chunks</button>
      <span id="chunksStatus"></span>
    </div>
  </div>
  
  <div id="chunks-container">
    <h3>Detected Audio Chunks</h3>
    <div id="chunks-info"></div>
  </div>

  <div class="progress-container" id="progressContainer">
    <p>Processing: <span id="progressPercent">0%</span></p>
    <progress id="progressBar" value="0" max="100"></progress>
  </div>

  <button id="transcribeAllChunksBtn" class="button-primary" disabled>Transcribe All Chunks</button>
  <button id="translateBtn" class="button-primary" disabled>Translate</button>

  <div class="download-section">
    <h2>Downloads</h2>
    <button id="downloadSrtBtn" class="button-disabled" disabled>Download Transcript (.srt)</button>
    <button id="downloadTranslatedSrtBtn" class="button-disabled" disabled>Download Translation (.srt)</button>
  </div>

  <h2>Log</h2>
  <div id="log"></div>

  <script>
    let originalSrt = '';
    let translatedSrt = '';
    let wavesurfer = null;
    let regionsPlugin = null;
    let detectedChunks = [];
    let audioContext = null;
    let audioBuffer = null;
    let isProcessingChunks = false;
    
    const logDiv = document.getElementById('log');
    const progressContainer = document.getElementById('progressContainer');
    const progressBar = document.getElementById('progressBar');
    const progressPercent = document.getElementById('progressPercent');
    const downloadSrtBtn = document.getElementById('downloadSrtBtn');
    const downloadTranslatedSrtBtn = document.getElementById('downloadTranslatedSrtBtn');
    const chunksContainer = document.getElementById('chunks-container');
    const chunksInfo = document.getElementById('chunks-info');
    const detectChunksBtn = document.getElementById('detectChunksBtn');
    const transcribeAllChunksBtn = document.getElementById('transcribeAllChunksBtn');
    const silenceThreshold = document.getElementById('silenceThreshold');
    const silenceThresholdValue = document.getElementById('silenceThresholdValue');
    const minSilenceDuration = document.getElementById('minSilenceDuration');
    const minSilenceDurationValue = document.getElementById('minSilenceDurationValue');
    const chunksStatus = document.getElementById('chunksStatus');

    // Update slider values display
    silenceThreshold.addEventListener('input', () => {
      silenceThresholdValue.textContent = `${silenceThreshold.value} dB`;
    });
    
    minSilenceDuration.addEventListener('input', () => {
      minSilenceDurationValue.textContent = `${minSilenceDuration.value} ms`;
    });

    function log(msg) {
      logDiv.textContent += msg + '\n';
      logDiv.scrollTop = logDiv.scrollHeight;
    }
    
    function formatTime(seconds) {
      const minutes = Math.floor(seconds / 60);
      const secs = Math.floor(seconds % 60);
      return `${minutes}:${secs.toString().padStart(2, '0')}`;
    }
    
    function formatTimeExact(seconds) {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const ms = Math.floor((seconds % 1) * 1000);
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
    }
    
    // Initialize WaveSurfer with v6 API
    function initWaveSurfer() {
      // Create the regions plugin
      regionsPlugin = WaveSurfer.regions.create();
      
      // Create wavesurfer instance
      wavesurfer = WaveSurfer.create({
        container: '#waveform',
        waveColor: '#3498db',
        progressColor: '#2980b9',
        cursorColor: '#333',
        barWidth: 2,
        barRadius: 3,
        cursorWidth: 1,
        height: 100,
        barGap: 2,
        responsive: true,
        plugins: [regionsPlugin]
      });
      
      // Enable controls once audio is loaded
      wavesurfer.on('ready', function() {
        document.getElementById('playBtn').disabled = false;
        document.getElementById('pauseBtn').disabled = false;
        document.getElementById('zoomInBtn').disabled = false;
        document.getElementById('zoomOutBtn').disabled = false;
        detectChunksBtn.disabled = false;
      });
    }
    
    // Function to clear all regions in WaveSurfer
    function clearAllRegions() {
      if (regionsPlugin) {
        // Use clearRegions() method in WaveSurfer.js v6
        regionsPlugin.clearRegions();
      }
    }

    // Handle file selection
    document.getElementById('mediaFile').addEventListener('change', async function(e) {
      const file = e.target.files[0];
      if (!file) return;
      
      // Reset chunks
      detectedChunks = [];
      chunksContainer.style.display = 'none';
      transcribeAllChunksBtn.disabled = true;
      
      // If wavesurfer hasn't been initialized yet, initialize it
      if (!wavesurfer) {
        initWaveSurfer();
      } else {
        // Clear existing regions
        clearAllRegions();
      }
      
      // Load the file into wavesurfer
      const fileURL = URL.createObjectURL(file);
      wavesurfer.load(fileURL);
      log(`Loaded file: ${file.name}`);
      
      // Also load the file into an AudioBuffer for analysis
      try {
        if (!audioContext) {
          audioContext = new (window.AudioContext || window.webkitAudioContext)();
        }
        
        const arrayBuffer = await file.arrayBuffer();
        audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
        log('Audio data loaded for analysis');
      } catch (error) {
        log(`Error loading audio for analysis: ${error.message}`);
      }
    });
    
    // Audio control buttons
    document.getElementById('playBtn').addEventListener('click', () => {
      wavesurfer.play();
    });
    
    document.getElementById('pauseBtn').addEventListener('click', () => {
      wavesurfer.pause();
    });
    
    document.getElementById('zoomInBtn').addEventListener('click', () => {
      const currentZoom = wavesurfer.params.minPxPerSec || 1;
      wavesurfer.zoom(currentZoom * 1.5);
    });
    
    document.getElementById('zoomOutBtn').addEventListener('click', () => {
      const currentZoom = wavesurfer.params.minPxPerSec || 1;
      wavesurfer.zoom(Math.max(1, currentZoom / 1.5));
    });
    
    // Function to detect silence for intelligent chunking
    async function detectSilence() {
      if (!audioBuffer) {
        log('No audio loaded for analysis');
        return [];
      }
      
      chunksStatus.textContent = 'Analyzing...';
      
      // Get raw audio data (we'll use the first channel if stereo)
      const audioData = audioBuffer.getChannelData(0);
      const sampleRate = audioBuffer.sampleRate;
      
      // Get threshold values from UI sliders
      const thresholdDb = parseInt(silenceThreshold.value);
      const minSilenceMs = parseInt(minSilenceDuration.value);
      
      // Convert from dB to amplitude ratio (0 to 1)
      const thresholdAmplitude = Math.pow(10, thresholdDb / 20);
      
      // Convert from ms to samples
      const minSilenceSamples = Math.floor(minSilenceMs * sampleRate / 1000);
      
      const silencePoints = [];
      let inSilence = false;
      let silenceStart = 0;
      let consecutiveSilenceSamples = 0;
      
      // Process in chunks to not freeze the UI
      const CHUNK_SIZE = 50000;
      let processedSamples = 0;
      
      while (processedSamples < audioData.length) {
        const endIndex = Math.min(processedSamples + CHUNK_SIZE, audioData.length);
        
        for (let i = processedSamples; i < endIndex; i++) {
          const amplitude = Math.abs(audioData[i]);
          
          if (amplitude < thresholdAmplitude) {
            // We're in a silent part
            if (!inSilence) {
              // Just entered silence
              silenceStart = i;
              inSilence = true;
            }
            consecutiveSilenceSamples++;
          } else {
            // We're in a non-silent part
            if (inSilence) {
              // Just exited silence, check if it was long enough
              if (consecutiveSilenceSamples >= minSilenceSamples) {
                // This silence was long enough, record its position
                const silenceMiddle = silenceStart + (consecutiveSilenceSamples / 2);
                silencePoints.push(silenceMiddle / sampleRate);
              }
              inSilence = false;
            }
            consecutiveSilenceSamples = 0;
          }
        }
        
        processedSamples = endIndex;
        
        // Let the UI update
        await new Promise(resolve => setTimeout(resolve, 0));
        chunksStatus.textContent = `Analyzing... ${Math.round(processedSamples / audioData.length * 100)}%`;
      }
      
      // In case we end in silence
      if (inSilence && consecutiveSilenceSamples >= minSilenceSamples) {
        const silenceMiddle = silenceStart + (consecutiveSilenceSamples / 2);
        silencePoints.push(silenceMiddle / sampleRate);
      }
      
      return silencePoints;
    }
    
    // Function to create optimized chunks that stay under 25MB
    async function createOptimizedChunks() {
      if (!audioBuffer || !wavesurfer) {
        return [];
      }
      
      // Find all silence points
      const silencePoints = await detectSilence();
      
      if (silencePoints.length === 0) {
        log('No significant pauses detected. Try adjusting the silence threshold or minimum duration.');
        return [];
      }
      
      log(`Detected ${silencePoints.length} potential break points`);
      
      // Get the audio duration
      const totalDuration = audioBuffer.duration;
      
      // Estimate the bytes per second rate (rough approximation)
      // Assuming 16bit stereo at 44.1kHz for WAV
      const bytesPerSecond = 44100 * 4; // 44.1kHz * (2 channels * 2 bytes per sample)
      
      // Target size in bytes (slightly under 25MB to be safe)
      const targetChunkSizeBytes = 23 * 1024 * 1024;
      
      // Calculate target duration per chunk
      const targetChunkDuration = targetChunkSizeBytes / bytesPerSecond;
      
      // Create chunks
      const chunks = [];
      let chunkStart = 0;
      
      while (chunkStart < totalDuration) {
        // Target end time for this chunk (min of target duration or end)
        const idealEnd = Math.min(chunkStart + targetChunkDuration, totalDuration);
        
        // Find the closest silence point to the ideal end
        let bestBreakPoint = idealEnd;
        let minDistance = Number.MAX_VALUE;
        
        for (const silencePoint of silencePoints) {
          // Only consider silence points past our starting point and not too far past ideal end
          if (silencePoint > chunkStart && silencePoint < idealEnd + 30) {
            const distance = Math.abs(silencePoint - idealEnd);
            if (distance < minDistance) {
              minDistance = distance;
              bestBreakPoint = silencePoint;
            }
          }
        }
        
        // If we couldn't find a good break point, just use the ideal end
        if (bestBreakPoint === idealEnd && idealEnd < totalDuration) {
          log(`Warning: No natural pause found near the ${formatTime(idealEnd)} mark`);
        }
        
        // Add the chunk
        chunks.push({
          start: chunkStart,
          end: bestBreakPoint,
          duration: bestBreakPoint - chunkStart
        });
        
        // Move to next chunk
        chunkStart = bestBreakPoint;
      }
      
      return chunks;
    }
    
    // Show the detected chunks and visualize them
    function displayChunks(chunks) {
      if (!chunks || chunks.length === 0) {
        chunksContainer.style.display = 'none';
        return;
      }
      
      chunksContainer.style.display = 'block';
      chunksInfo.innerHTML = '';
      
      let totalHTML = '<table style="width:100%; border-collapse: collapse;">';
      totalHTML += '<tr><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Chunk</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Start</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">End</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Duration</th><th style="text-align:left; border-bottom:1px solid #ddd; padding:5px;">Est. Size</th></tr>';
      
      // Add markers to the waveform
      clearAllRegions();
      
      chunks.forEach((chunk, index) => {
        // Add region for this chunk if plugin is available
        if (regionsPlugin) {
          regionsPlugin.addRegion({
            start: chunk.start,
            end: chunk.end,
            color: 'rgba(76, 175, 80, 0.2)',
            id: `chunk_${index}`
          });
        }
        
        // Add a vertical marker for chunk boundaries (except start)
        if (index > 0) {
          const marker = document.createElement('div');
          marker.className = 'chunk-marker';
          marker.style.left = (chunk.start / wavesurfer.getDuration() * 100) + '%';
          document.querySelector('#waveform').appendChild(marker);
        }
        
        // Calculate estimated size
        const estSizeBytes = chunk.duration * 44100 * 4; // 44.1kHz * (2 channels * 2 bytes per sample)
        const estSizeMB = estSizeBytes / (1024 * 1024);
        
        // Add row to table
        totalHTML += `<tr>
          <td style="border-bottom:1px solid #ddd; padding:5px;">Chunk ${index + 1}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.start)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.end)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${formatTime(chunk.duration)}</td>
          <td style="border-bottom:1px solid #ddd; padding:5px;">${estSizeMB.toFixed(2)} MB</td>
        </tr>`;
      });
      
      totalHTML += '</table>';
      chunksInfo.innerHTML = totalHTML;
      
      // Enable the "Transcribe All Chunks" button
      transcribeAllChunksBtn.disabled = false;
    }
    
    detectChunksBtn.addEventListener('click', async () => {
      if (!audioBuffer || !wavesurfer) {
        log('Please load an audio file first');
        return;
      }
      
      // Remove existing chunk markers
      document.querySelectorAll('.chunk-marker').forEach(marker => marker.remove());
      
      log('Detecting optimal chunk boundaries...');
      const chunks = await createOptimizedChunks();
      
      if (chunks.length > 0) {
        log(`Created ${chunks.length} optimized chunks based on silence detection`);
        detectedChunks = chunks;
        displayChunks(chunks);
        chunksStatus.textContent = `${chunks.length} chunks detected`;
      } else {
        chunksStatus.textContent = 'No chunks detected';
      }
    });
    
    // Extract audio for a specific chunk
    async function extractChunkAudio(chunk) {
      if (!wavesurfer || !audioBuffer) {
        return null;
      }
      
      try {
        // Create a new AudioContext for processing
        const context = new (window.AudioContext || window.webkitAudioContext)();
        
        // Calculate start and end samples
        const sampleRate = audioBuffer.sampleRate;
        const startSample = Math.floor(chunk.start * sampleRate);
        const endSample = Math.floor(chunk.end * sampleRate);
        const length = endSample - startSample;
        
        // Create a new buffer for the chunk
        const chunkBuffer = context.createBuffer(
          audioBuffer.numberOfChannels,
          length,
          sampleRate
        );
        
        // Copy data from the original buffer to the chunk buffer
        for (let channel = 0; channel < audioBuffer.numberOfChannels; channel++) {
          const originalData = audioBuffer.getChannelData(channel);
          const chunkData = chunkBuffer.getChannelData(channel);
          
          for (let i = 0; i < length; i++) {
            chunkData[i] = originalData[startSample + i];
          }
        }
        
        // Convert to WAV
        const offlineContext = new OfflineAudioContext(
          audioBuffer.numberOfChannels,
          length,
          sampleRate
        );
        
        const source = offlineContext.createBufferSource();
        source.buffer = chunkBuffer;
        source.connect(offlineContext.destination);
        source.start(0);
        
        const renderedBuffer = await offlineContext.startRendering();
        
        // Convert to blob
        const wavBlob = await bufferToWav(renderedBuffer);
        return wavBlob;
      } catch (error) {
        log(`Error extracting chunk: ${error.message}`);
        return null;
      }
    }
    
    // Helper function to convert AudioBuffer to WAV blob
    function bufferToWav(buffer) {
      return new Promise(resolve => {
        const numberOfChannels = buffer.numberOfChannels;
        const sampleRate = buffer.sampleRate;
        const length = buffer.length;
        
        // Create the WAV file
        const wavFile = new ArrayBuffer(44 + length * numberOfChannels * 2);
        const view = new DataView(wavFile);
        
        // Write the WAV header
        // "RIFF" chunk descriptor
        writeString(view, 0, 'RIFF');
        view.setUint32(4, 36 + length * numberOfChannels * 2, true);
        writeString(view, 8, 'WAVE');
        
        // "fmt " sub-chunk
        writeString(view, 12, 'fmt ');
        view.setUint32(16, 16, true); // subchunk1size
        view.setUint16(20, 1, true); // audio format (1 = PCM)
        view.setUint16(22, numberOfChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numberOfChannels * 2, true); // byte rate
        view.setUint16(32, numberOfChannels * 2, true); // block align
        view.setUint16(34, 16, true); // bits per sample
        
        // "data" sub-chunk
        writeString(view, 36, 'data');
        view.setUint32(40, length * numberOfChannels * 2, true);
        
        // Write the PCM samples
        const offset = 44;
        let pos = 0;
        for (let i = 0; i < length; i++) {
          for (let channel = 0; channel < numberOfChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, buffer.getChannelData(channel)[i]));
            view.setInt16(offset + pos, sample < 0 ? sample * 0x8000 : sample * 0x7FFF, true);
            pos += 2;
          }
        }
        
        // Create the Blob
        const blob = new Blob([wavFile], { type: 'audio/wav' });
        resolve(blob);
      });
      
      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }
    }
    
    // Transcribe all chunks and merge them
    async function transcribeAllChunks() {
      if (isProcessingChunks) {
        alert('Already processing chunks. Please wait.');
        return;
      }
      
      if (!detectedChunks || detectedChunks.length === 0) {
        alert('No chunks detected. Please run the chunk detection first.');
        return;
      }
      
      const apiKey = document.getElementById('apiKey').value.trim();
      if (!apiKey) {
        alert('Please provide an OpenAI API key.');
        return;
      }
      
      isProcessingChunks = true;
      
      try {
        const allChunksSrt = [];
        progressContainer.style.display = 'block';
        
        for (let i = 0; i < detectedChunks.length; i++) {
          const chunk = detectedChunks[i];
          log(`Processing chunk ${i+1}/${detectedChunks.length} (${formatTime(chunk.start)} - ${formatTime(chunk.end)})...`);
          
          // Extract the audio for this chunk
          const chunkAudio = await extractChunkAudio(chunk);
          
          if (!chunkAudio) {
            log(`Failed to extract audio for chunk ${i+1}`);
            continue;
          }
          
          log(`Chunk ${i+1} size: ${Math.round(chunkAudio.size / 1024 / 1024 * 100) / 100} MB`);
          
          // Create form data for this chunk
          const form = new FormData();
          form.append('file', chunkAudio, `chunk${i+1}.wav`);
          form.append('model', 'whisper-1');
          form.append('response_format', 'srt');
          
          // Send to Whisper API
          log(`Transcribing chunk ${i+1}...`);
          progressPercent.textContent = `Chunk ${i+1}/${detectedChunks.length}`;
          progressBar.value = (i / detectedChunks.length) * 100;
          
          // Use fetch for simplicity
          const response = await fetch('https://api.openai.com/v1/audio/transcriptions', {
            method: 'POST',
            headers: {
              'Authorization': `Bearer ${apiKey}`
            },
            body: form
          });
          
          if (!response.ok) {
            const errorText = await response.text();
            log(`Error transcribing chunk ${i+1}: ${errorText}`);
            continue;
          }
          
          const chunkSrt = await response.text();
          
          // Store the SRT with timing information
          allChunksSrt.push({
            srt: chunkSrt,
            offset: chunk.start
          });
          
          log(`Chunk ${i+1} transcribed successfully`);
          
          // Wait a moment between chunks to avoid rate limiting
          await new Promise(resolve => setTimeout(resolve, 500));
        }
        
        // Merge all SRTs with correct timings
        log('Merging all transcriptions...');
        originalSrt = mergeSubtitles(allChunksSrt);
        
        log('All chunks processed and merged successfully');
        
        // Enable buttons
        downloadSrtBtn.disabled = false;
        downloadSrtBtn.classList.remove('button-disabled');
        downloadSrtBtn.classList.add('button-secondary');
        document.getElementById('translateBtn').disabled = false;
        
      } catch (error) {
        log(`Error processing chunks: ${error.message}`);
      } finally {
        isProcessingChunks = false;
        progressContainer.style.display = 'none';
      }
    }
    
    // Merge multiple SRT files with proper timing offsets
    function mergeSubtitles(srtChunks) {
      if (!srtChunks || srtChunks.length === 0) return '';
      
      let allSubs = [];
      let subCounter = 1;
      
      // Process each chunk
      srtChunks.forEach(chunk => {
        const { srt, offset } = chunk;
        
        // Parse the SRT content
        const subLines = srt.trim().split('\n\n');
        
        // Process each subtitle in this chunk
        for (const sub of subLines) {
          const lines = sub.trim().split('\n');
          
          // Skip empty subtitles
          if (lines.length < 3) continue;
          
          // Skip subtitle number (lines[0])
          const timeStr = lines[1];
          
          // Parse start and end times
          const times = timeStr.split(' --> ');
          if (times.length !== 2) continue;
          
          const startTime = parseSrtTime(times[0]) + offset;
          const endTime = parseSrtTime(times[1]) + offset;
          
          // Create adjusted subtitle
          const text = lines.slice(2).join('\n');
          
          allSubs.push({
            number: subCounter++,
            start: startTime,
            end: endTime,
            text: text
          });
        }
      });
      
      // Sort subtitles by start time
      allSubs.sort((a, b) => a.start - b.start);
      
      // Format as SRT
      let result = '';
      for (const sub of allSubs) {
        result += `${sub.number}\n`;
        result += `${formatSrtTime(sub.start)} --> ${formatSrtTime(sub.end)}\n`;
        result += `${sub.text}\n\n`;
      }
      
      return result;
    }
    
    // Parse SRT time format to seconds
    function parseSrtTime(timeStr) {
      const [time, ms] = timeStr.split(',');
      const [hours, minutes, seconds] = time.split(':').map(Number);
      return hours * 3600 + minutes * 60 + seconds + parseInt(ms) / 1000;
    }
    
    // Format seconds to SRT time format
    function formatSrtTime(seconds) {
      const hours = Math.floor(seconds / 3600);
      const minutes = Math.floor((seconds % 3600) / 60);
      const secs = Math.floor(seconds % 60);
      const ms = Math.floor((seconds % 1) * 1000);
      
      return `${hours.toString().padStart(2, '0')}:${minutes.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')},${ms.toString().padStart(3, '0')}`;
    }

    async function translateSrt() {
      const apiKey = document.getElementById('apiKey').value.trim();
      if (!originalSrt) return;
      
      log('Translating SRT with GPT-4o...');
      const systemMsg = 'You are a helpful assistant that translates subtitles while preserving the timing and SRT structure exactly.';
      const userMsg = `Translate the following SRT to the target language, preserving numbering and timestamps exactly:\n\n${originalSrt}`;
      const payload = {
        model: 'gpt-4o', 
        messages: [
          { role: 'system', content: systemMsg },
          { role: 'user', content: userMsg }
        ], 
        max_tokens: 20000
      };
      const resp = await fetch('https://api.openai.com/v1/chat/completions', {
        method: 'POST', 
        headers: {
          'Content-Type': 'application/json', 
          'Authorization': `Bearer ${apiKey}`
        }, 
        body: JSON.stringify(payload)
      });
      if (!resp.ok) {
        const err = await resp.text(); 
        log('Error: ' + err); 
        return;
      }
      const data = await resp.json();
      translatedSrt = data.choices[0].message.content;
      log('Translation done.');
      
      // Enable the download button
      downloadTranslatedSrtBtn.disabled = false;
      downloadTranslatedSrtBtn.classList.remove('button-disabled');
      downloadTranslatedSrtBtn.classList.add('button-secondary');
    }
    
    // Download buttons event listeners
    downloadSrtBtn.addEventListener('click', () => {
      if (!originalSrt) return;
      
      const url = URL.createObjectURL(new Blob([originalSrt], { type: 'text/plain' }));
      const a = document.createElement('a');
      a.href = url;
      a.download = 'transcript.srt';
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
    });
    
    downloadTranslatedSrtBtn.addEventListener('click', () => {
      if (!translatedSrt) return;
      
      const url = URL.createObjectURL(new Blob([translatedSrt], { type: 'text/plain' }));
      const a = document.createElement('a');
      a.href = url;
      a.download = 'translated.srt';
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
    });

    document.getElementById('transcribeAllChunksBtn').addEventListener('click', transcribeAllChunks);
    document.getElementById('translateBtn').addEventListener('click', translateSrt);
  </script>
</body>
</html>
